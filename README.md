# âœ‹ **ASL Hand Gesture Recognition**

This project uses **MediaPipe Hands** + **Machine Learning (Random Forest)** to recognize American Sign Language alphabet gestures (Aâ€“Z) from a webcam feed.

It includes:

- Data preprocessing

- Model training

- Real-time prediction using webcam

**Link** : [Official MediaPipe Face Mesh documentation](https://mediapipe.readthedocs.io/en/latest/solutions/hands.html)

## ğŸ“ **Repository Structure**
```bash
â”œâ”€â”€ README.md              # Project documentation
â”œâ”€â”€ main.py                # Real-time hand gesture recognition (webcam)
â”œâ”€â”€ create_data.py         # Create your own data
â”œâ”€â”€ model.py               # Train the RandomForest model
â”œâ”€â”€ clean_data.py          # Extract landmarks from images â†’ data.pickle
â”œâ”€â”€ data.pickle            # Preprocessed dataset (landmarks)
â””â”€â”€ model.p                # Trained RandomForest model
```

## ğŸš€ **Features**

- Detects a hand using **MediaPipe**

- Extracts **21 hand landmarks** (x/y coordinates)

- **Normalizes** data for ML training

- Predicts ASL letters Aâ€“Z live from **webcam**

- **Lightweight** (no deep learning required)

- Works in **real time**

<img width="640" height="338" alt="Image" src="https://github.com/user-attachments/assets/d3b8d396-1e40-48e5-a14d-d1304e2f0ff0" />

## ğŸ›  **Installation**
1ï¸âƒ£ Clone the repo
```bash
git clone https://github.com/MohamedAli1937/Sign-Language-Detection.git
```

2ï¸âƒ£ Install dependencies
```bash
pip install opencv-python mediapipe scikit-learn numpy tqdm
```
## ğŸ“¸ **Collecting Your Own Dataset**

Run the script to capture images for each alphabet letter:
```python
python clean_data.py
```


This processes your raw images inside `data/` and generates:
`data.pickle`

## ğŸ§  **Training the Model**

Train the **RandomForest classifier** using:
```python
python model.py
```

This produces: `model.p`

## ğŸ¥ **Real-Time Hand Recognition**

Launch live prediction with your webcam:
```python
python main.py
```

Controls:
`ESC` â†’ quit

## ğŸ§± **How It Works (Simplified)**

1ï¸âƒ£ **MediaPipe** detects the hand

2ï¸âƒ£ Extract **21 landmarks** â†’ (x,y) â†’ 42 features

3ï¸âƒ£ **Normalize** landmarks relative to the **minimum x, y** (same during training + testing)

4ï¸âƒ£ **RandomForest** predicts a class 0â€“25 â†’ mapped to Aâ€“Z

## ğŸ“Œ **Requirements**

**Python 3.8+** & **Webcam** & **Good lighting for best performance**

## ğŸ™Œ **Future Improvements**

- Add smoothing filter to stabilize predictions

- Add gesture recording for custom signs

- Convert to CNN for higher accuracy

- Build a simple Tkinter or web UI
